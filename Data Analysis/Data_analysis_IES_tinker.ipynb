{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Term Structure Forecasting\n",
    "## Data Analysis\n",
    "#### David Koubek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model_fit.R all code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Master Thesis\n",
    "# Model fitting follows\n",
    "# v1.2 - converting the environment/code for cuDNN LSTM GPU training (WIP)\n",
    "\n",
    "rm(list = ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: pacman\n",
      "Warning message:\n",
      "\"package 'pacman' was built under R version 3.6.3\""
     ]
    }
   ],
   "source": [
    "# Bind R to a Python environment that has TensorFlow with GPU support\n",
    "# Do this first before loading any further packages, otherwise the R session can be binded with a default CPU environment and you will have to restart R to rebind\n",
    "# reticulate::use_condaenv(\"r-tensorflow-gpu\") # Specify the name of a conda environment.\n",
    "# reticulate::use_condaenv(\"r-tensorflow-gpu\", required = TRUE)\n",
    "# play:\n",
    "# reticulate::repl_python()\n",
    "# import tensorflow as tf\n",
    "\n",
    "# Packages\n",
    "if (!require(\"pacman\")) install.packages(\"pacman\") # installs pacman package if not installed yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pacman::p_load(pacman, dplyr, tidyr, ggplot2, scales, Cairo, zoo, xtable, tibble, forecast, naturalsort) # load packages TBD e.g. these, edit when u determine which needed/used..\n",
    "p_load(reticulate, tensorflow, keras)\n",
    "py_config()\n",
    "tf_config()\n",
    "# devtools::session_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered S3 methods overwritten by 'ggplot2':\n",
      "  method         from \n",
      "  [.quosures     rlang\n",
      "  c.quosures     rlang\n",
      "  print.quosures rlang\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Workspace rdy set go!\"\n"
     ]
    }
   ],
   "source": [
    "# Workspace\n",
    "# setwd(\"E:/Google_Drive/Diploma_Thesis/Code\")\n",
    "setwd(\"I:/Diploma_Thesis/Code\")\n",
    "# load(file = \"Workspaces/Data_03_trimmed-small_US.RData\") # 0min\n",
    "load(file = \"Workspaces/Data_04_split-small_US_tick_05-2019.RData\") # 0min\n",
    "print(\"Workspace rdy set go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'US'"
      ],
      "text/latex": [
       "'US'"
      ],
      "text/markdown": [
       "'US'"
      ],
      "text/plain": [
       "[1] \"US\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'US'"
      ],
      "text/latex": [
       "'US'"
      ],
      "text/markdown": [
       "'US'"
      ],
      "text/plain": [
       "[1] \"US\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############################\n",
    "###### Split datasets #######\n",
    "#############################\n",
    "\n",
    "# Picks the future\n",
    "futurenames # shall contain US and EU various maturities eventually\n",
    "future <- 1\n",
    "futurenames[future]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "780735"
      ],
      "text/latex": [
       "780735"
      ],
      "text/markdown": [
       "780735"
      ],
      "text/plain": [
       "[1] 780735"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "468441"
      ],
      "text/latex": [
       "468441"
      ],
      "text/markdown": [
       "468441"
      ],
      "text/plain": [
       "[1] 468441"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "624588"
      ],
      "text/latex": [
       "624588"
      ],
      "text/markdown": [
       "624588"
      ],
      "text/plain": [
       "[1] 624588"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n",
       "-0.1250000  0.0000000  0.0000000  0.0000044  0.0000000  0.5625000 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train and test split\n",
    "# dataFutures_tmp <- data_small_aday$Close # one day of data, 24k obs\n",
    "dataFutures_tmp <- data_small_amonth$Close # one month of data, 780k obs\n",
    "(end <- length(dataFutures_tmp))\n",
    "(split_train <- round(3/5 * end))\n",
    "(split_val <- round(4/5 * end))\n",
    "\n",
    "dataFutures_train <- dataFutures_tmp[1:split_train]\n",
    "dataFutures_val <- dataFutures_tmp[(split_train + 1):split_val]\n",
    "dataFutures_test <- dataFutures_tmp[(split_val + 1):end]\n",
    "\n",
    "# Differencing the data - optional, but better results\n",
    "dataFutures_train_orig <- dataFutures_train # backup of original data for the end \"undiff\"\n",
    "dataFutures_val_orig <- dataFutures_val\n",
    "dataFutures_test_orig <- dataFutures_test\n",
    "\n",
    "dataFutures_train <- diff(dataFutures_train)\n",
    "dataFutures_val <- diff(dataFutures_val)\n",
    "dataFutures_test <- diff(dataFutures_test)\n",
    "summary(dataFutures_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Mode   FALSE    TRUE \n",
       "logical  445208   23232 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(dataFutures_train != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " num [1:23232] -0.0312 0.0312 -0.0312 0.0312 -0.0312 ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n",
       "-0.1250000 -0.0312500  0.0312500  0.0000888  0.0312500  0.5625000 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timpy <- dataFutures_train[dataFutures_train != 0]\n",
    "str(timpy)\n",
    "summary(timpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'HistogramTools' is in use and will not be installed\""
     ]
    }
   ],
   "source": [
    "install.packages(\"HistogramTools\")\n",
    "library(HistogramTools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in stopifnot(is.numeric(breaks), is.numeric(counts)): argument \"counts\" is missing, with no default\n",
     "output_type": "error",
     "traceback": [
      "Error in stopifnot(is.numeric(breaks), is.numeric(counts)): argument \"counts\" is missing, with no default\nTraceback:\n",
      "1. HistogramTools:::.BuildHistogram(timpy)",
      "2. stopifnot(is.numeric(breaks), is.numeric(counts))"
     ]
    }
   ],
   "source": [
    "myhist <- HistogramTools:::.BuildHistogram(timpy)\n",
    "plot(myhist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(timpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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",
      "text/plain": [
       "Plot with title \"Histogram of timpy\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist(timpy, breaks = 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#############################\n",
    "###### Graph datasets #######\n",
    "#############################\n",
    "\n",
    "# Graph of the split original dataset\n",
    "\tdata_df <- data_frame(time = 1:(length(dataFutures_train_orig) + length(dataFutures_val_orig) + length(dataFutures_test_orig)),\n",
    "\t                 \t  train = c(dataFutures_train_orig, rep(NA, length(dataFutures_val_orig) + length(dataFutures_test_orig))),\n",
    "\t                 \t  val = c(rep(NA, length(dataFutures_train_orig)), dataFutures_val_orig, rep(NA, length(dataFutures_test_orig))),\n",
    "\t                 \t  test = c(rep(NA, length(dataFutures_train_orig) + length(dataFutures_val_orig)), dataFutures_test_orig))\n",
    "\tdata_df <- data_df %>% gather(key = 'train_val_test', value = 'value', -time) # -time\n",
    "\n",
    "\tcustom_scale <- 1.5 # for the graph to look nice, sizes of graph items and fonts\n",
    "\tdataset_graph <- ggplot(data_df, aes(x = time, y = value, color = train_val_test)) +\n",
    "\t\tgeom_line() +\n",
    "\t\tlabs(x = \"obs\", y = \"price\") +\n",
    "\t\t# scale_x_continuous(breaks = c(1, round(seq(20000, end - 3, by = 20000), 1), split_train - 1, split_val - 2, end - 3)) +\n",
    "\t\t# scale_x_continuous(breaks = pretty_breaks(5)) +\n",
    "\t\tscale_colour_discrete(name = \"Dataset\", breaks = c(\"train\", \"val\", \"test\"), labels = c(\"train\", \"validate\", \"test\")) +\n",
    "\t\tguides(colour = guide_legend(override.aes = list(size = 4/3 * custom_scale))) +\n",
    "\t\ttheme_bw(base_family = \"LM Roman 10\", base_size = 16 * custom_scale) +\n",
    "\t\ttheme(legend.key.height = unit(1 * custom_scale, \"lines\"))\n",
    "\t\t# theme(legend.key.height = unit(1 * custom_scale, \"lines\"), axis.text.x = element_text(angle = 35, vjust = 0.8))\n",
    "\tdataset_graph\n",
    "\n",
    "# Graph of the split differenced dataset\n",
    "\tdata_df <- data_frame(time = 1:(length(dataFutures_train) + length(dataFutures_val) + length(dataFutures_test)),\n",
    "\t                 \t  train = c(dataFutures_train, rep(NA, length(dataFutures_val) + length(dataFutures_test))),\n",
    "\t                 \t  val = c(rep(NA, length(dataFutures_train)), dataFutures_val, rep(NA, length(dataFutures_test))),\n",
    "\t                 \t  test = c(rep(NA, length(dataFutures_train) + length(dataFutures_val)), dataFutures_test))\n",
    "\n",
    "\tdata_df <- data_df %>% gather(key = 'train_val_test', value = 'value', -time) # -time\n",
    "\n",
    "\tcustom_scale <- 1.5 # for the graph to look nice, sizes of graph items and fonts\n",
    "\tsplit_graph <- ggplot(data_df, aes(x = time, y = value, color = train_val_test)) +\n",
    "\t\tgeom_line() +\n",
    "\t\tlabs(x = \"obs\", y = \"price difference\") +\n",
    "\t\t# scale_x_continuous(breaks = c(1, round(seq(20000, end - 3, by = 20000), 1), split_train - 1, split_val - 2, end - 3)) +\n",
    "\t\t# scale_x_continuous(breaks = pretty_breaks(5)) +\n",
    "\t\tscale_colour_discrete(name = \"Dataset\", breaks = c(\"train\", \"val\", \"test\"), labels = c(\"train\", \"validate\", \"test\")) +\n",
    "\t\tguides(colour = guide_legend(override.aes = list(size = 4/3 * custom_scale))) +\n",
    "\t\ttheme_bw(base_family = \"LM Roman 10\", base_size = 16 * custom_scale) +\n",
    "\t\ttheme(legend.key.height = unit(1 * custom_scale, \"lines\"))\n",
    "\t\t# theme(legend.key.height = unit(1 * custom_scale, \"lines\"), axis.text.x = element_text(angle = 35, vjust = 0.8))\n",
    "\tsplit_graph\n",
    "\n",
    "# Save the graphs to file\n",
    "\tGoldenRatio <- (1 + sqrt(5)) / 2\n",
    "\t# plots_width <- 5.55226 # width in inches of thesis template textwidth\n",
    "\tplots_width <- 10 * custom_scale # 10 inches plus have nicely smallish graph elements, adjust custom scale for each graph type what looks nice\n",
    "\tplots_height <- plots_width / GoldenRatio\n",
    "\n",
    "# A day\n",
    "\t# ggsave(dataset_graph, filename = paste0(\"Graphs/dataset_\", futurenames[future], \"_aday.pdf\"), device = cairo_pdf,\n",
    "\t# \twidth = plots_width, height = plots_height, units = \"in\")\n",
    "\t# ggsave(split_graph, filename = paste0(\"Graphs/differenced_\", futurenames[future], \"_aday.pdf\"), device = cairo_pdf,\n",
    "\t# \twidth = plots_width, height = plots_height, units = \"in\")\n",
    "\n",
    "# A month\n",
    "\tggsave(dataset_graph, filename = paste0(\"Graphs/dataset_\", futurenames[future], \"_amonth.pdf\"), device = cairo_pdf,\n",
    "\t\twidth = plots_width, height = plots_height, units = \"in\")\n",
    "\tggsave(split_graph, filename = paste0(\"Graphs/differenced_\", futurenames[future], \"_amonth.pdf\"), device = cairo_pdf,\n",
    "\t\twidth = plots_width, height = plots_height, units = \"in\")\n",
    "\n",
    "\n",
    "# Save workspace\n",
    "# save.image(file = paste0(\"Workspaces/Data_04_split-small_\", futurenames[future], \"_tick_05-2019.RData\")) # 1min 21MB\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############################\n",
    "#### TBD Factor models ######\n",
    "#############################\n",
    "\n",
    "# Not programmed yet, shall run the data through the factor models (DNS) before training the NNs.\n",
    "# In a separate script probably, Model_factor.R\n",
    "# Run script.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############################\n",
    "########## ANNs #############\n",
    "#############################\n",
    "\n",
    "ls()\n",
    "\n",
    "### Prepare parameters ###\n",
    "\n",
    "# Windowing the data\n",
    "lstm_num_timesteps <- 4\n",
    "# lstm_num_timesteps <- 12\n",
    "\n",
    "window_data <- function(data) { # window the input data\n",
    "\tt(sapply(1:(length(data) - lstm_num_timesteps), function(x) data[x:(x + lstm_num_timesteps - 1)]))\n",
    "}\n",
    "\n",
    "data_train <- window_data(dataFutures_train)\n",
    "data_val <- window_data(dataFutures_val)\n",
    "data_test <- window_data(dataFutures_test)\n",
    "data_train[1:5, 1:4]\n",
    "\n",
    "window_labels <- function(data) { # window the labels data\n",
    "\tsapply((lstm_num_timesteps + 1):(length(data)), function(x) data[x])\n",
    "}\n",
    "\n",
    "labels_train <- window_labels(dataFutures_train)\n",
    "labels_val <- window_labels(dataFutures_val)\n",
    "labels_test <- window_labels(dataFutures_test)\n",
    "labels_train[1:5]\n",
    "\n",
    "\n",
    "# Keras LSTMs expect the input array to be shaped as (no. samples, no. time steps, no. features), yet we have just the two axes, need to add an axis \"no.features\" at the end that will just say that we have 1 feature (we have just one column/price to feed)\n",
    "# dim(data_train)\n",
    "# dim(data_val)\n",
    "# dim(data_test)\n",
    "# K <- backend() # don't need this approach anymore, from Keras 2.1.2 can use k_expand_dims and k_eval\n",
    "# data_train <- K$eval(K$expand_dims(data_train, axis = 2L)) # 2L because of this https://github.com/rstudio/tensorflow/issues/190\n",
    "data_train <- k_eval(k_expand_dims(data_train, axis = -1)) # -1 for the last axis to expand at\n",
    "data_val <- k_eval(k_expand_dims(data_val, axis = -1))\n",
    "data_test <- k_eval(k_expand_dims(data_test, axis = -1))\n",
    "dim(data_train)\n",
    "str(data_train)\n",
    "class(data_train)\n",
    "\n",
    "\n",
    "# LSTM input shape: (samples, time steps, features)\n",
    "num_samples <- dim(data_train)[1]\n",
    "num_steps <- dim(data_train)[2]\n",
    "num_features <- dim(data_train)[3]\n",
    "c(num_samples, num_steps, num_features)\n",
    "\n",
    "\n",
    "\n",
    "#############################\n",
    "####### Training NNs ########\n",
    "#############################\n",
    "\n",
    "# TensorBoard initialisation/stopping\n",
    "# dir.create(\"TensorBoard_logs\")\n",
    "tensorboard(log_dir = \"TensorBoard_logs/tmp\", host = \"127.0.0.1\", port = \"1001\", launch_browser = utils::browseURL(\"http://127.0.0.1:1001\"), reload_interval = 5) # Launch TensorBoard and wait for output in specified directory\n",
    "# tensorboard(log_dir = \"TensorBoard_logs\", host = \"127.0.0.1\", port = \"1001\", reload_interval = 5) # Launch TensorBoard manually at http://127.0.0.1:1001 and wait for output in specified directory\n",
    "tensorboard(log_dir = \"TensorBoard_logs/tmp\", action = \"stop\") # stops the TensorBoard\n",
    "# browseURL(\"http://127.0.0.1:1001\") # launches the TensorBoard url\n",
    "\n",
    "##### TBD Tune #####\n",
    "# Define manually parameters here that will be used in the model\n",
    "### MAE ###\n",
    "\tn_epochs <- 5 # for now 50 takes 10min for smallest network, later obviously need to train longer to optimise until it starts overfitting\n",
    "\tbatch <- 128\n",
    "\n",
    "\tparameters_architecture <- list(nlayers = 1, units = c(8, 0, 0), dropout = 0.2, recurrent_dropout = 0.2) # 11s per epoch on a month of data\n",
    "\tparameters_compile <- list(loss = \"mae\", optimizer = \"rmsprop\", metrics = \"mse\") ### for MAE ###\n",
    "\t# parameters_compile <- list(loss = \"mse\", optimizer = \"rmsprop\", metrics = \"mae\") ### for MSE trials ###\n",
    "\tparameters_fit <- list(epochs = n_epochs, batch_size = batch)\n",
    "\tparameters_1 <- c(parameters_architecture, parameters_compile, parameters_fit)\n",
    "\n",
    "\tparameters_architecture <- list(nlayers = 1, units = c(32, 0, 0), dropout = 0.2, recurrent_dropout = 0.2)\n",
    "\t# parameters_fit <- list(epochs = n_epochs, batch_size = batch) # if we want to change sth\n",
    "\tparameters_2 <- c(parameters_architecture, parameters_compile, parameters_fit)\n",
    "\n",
    "\tparameters_architecture <- list(nlayers = 2, units = c(8, 16, 0), dropout = 0.1, recurrent_dropout = 0.5)\n",
    "\tparameters_3 <- c(parameters_architecture, parameters_compile, parameters_fit)\n",
    "\n",
    "\tparameters_architecture <- list(nlayers = 2, units = c(32, 64, 0), dropout = 0.1, recurrent_dropout = 0.5)\n",
    "\tparameters_4 <- c(parameters_architecture, parameters_compile, parameters_fit)\n",
    "\n",
    "\tparameters_architecture <- list(nlayers = 3, units = c(32, 64, 128), dropout = 0.1, recurrent_dropout = 0.5) # 43s per epoch\n",
    "\tparameters_5 <- c(parameters_architecture, parameters_compile, parameters_fit)\n",
    "\n",
    "\tparameters <- list(NN_1L_6N = parameters_1, NN_1L_32N = parameters_2,\n",
    "\t\t\t\t\t   NN_2L_6N12N = parameters_3, NN_2L_32N64N = parameters_4,\n",
    "\t\t\t\t\t   NN_3L_32N32N64N = parameters_5)\n",
    "\tstr(parameters)\n",
    "\n",
    "# Keras LSTM model\n",
    "# Looping the layers to automate it\n",
    "build_model_n <- function(nlayers = 1, units = rep(6, nlayers), dropout = 0, recurrent_dropout = 0,\n",
    "\t\t\t\t\t\t  loss = \"mae\", optimizer = \"rmsprop\", metrics = \"mse\", ...) {\n",
    "\tk_clear_session() # clearing the model/session first with backend to avoid TensorBoard errors\n",
    "\t\n",
    "\tmodel <- keras_model_sequential()\n",
    "\tfor (i in 1:nlayers) { # hidden layers\n",
    "\t\t# model %>% layer_cudnn_lstm(units = units[i], return_sequences = (i != nlayers), input_shape = c(num_steps, num_features))\n",
    "\t\t# AttributeError: module 'tensorflow_core.keras.layers' has no attribute 'CuDNNLSTM'\n",
    "\t\tmodel %>% layer_lstm(units = units[i], return_sequences = (i != nlayers), dropout = dropout, recurrent_dropout = recurrent_dropout, input_shape = c(num_steps, num_features))\n",
    "\t}\n",
    "\tmodel %>% layer_dense(units = 1) # output layer\n",
    "\n",
    "\tmodel %>% compile(\n",
    "\t\tloss = loss,\n",
    "\t\toptimizer = optimizer,\n",
    "\t\tmetrics = metrics\n",
    "\t)\n",
    "}\n",
    "\n",
    "save <- function(what, name) {\n",
    "\tsave_model_hdf5(what, paste0(\"Models/tmp/Model_\", name, \".h5\"))\n",
    "\tsave.image(file = paste0(\"Workspaces/tmp/Image_\", name, \"_trained.RData\"))\n",
    "}\n",
    "\n",
    "\n",
    "# model_names <- c()\n",
    "len <- 1\n",
    "# for (i in 1:len) {\n",
    "\ti <- 1 # number of the run\n",
    "\tprint(paste(\"Iteration\", i, \"/\", len))\n",
    "\tfor (hyp_tune in 1:1) { # 1:5 or 1:(length(parameters)\n",
    "\t\thyp_tune <- 1 # in case u want to skip the for loop and manually select which one to train\n",
    "\t\tprint(paste(\"Hyperparameter tuning\", hyp_tune, \"/\", length(parameters)))\n",
    "\n",
    "\t\tmodel <- do.call(what = build_model_n, args = parameters[[hyp_tune]])\n",
    "\t\tprint(model)\n",
    "\n",
    "\t\tlayers_name <- c()\n",
    "\t\tfor (j in 1:parameters[[hyp_tune]]$nlayers) {layers_name <- paste0(layers_name, model$layers[[j]]$units, \"N-\")}\n",
    "\t\tmodel_name <- paste0(futurenames[future], \"-1H-2003-2017-LSTM-k\", lstm_num_timesteps, \"-\", length(model$layers) - 1, \"L-\", layers_name,\n",
    "\t\t\t\t\t\t\t  100 * model$layers[[1]]$dropout, \"pDr-\", 100 * model$layers[[1]]$recurrent_dropout, \"pRDr-\",\n",
    "\t\t\t\t\t\t\t  toupper(model$loss),\"-\", parameters_compile$optimizer, \"-\", toupper(model$metrics_names[2]),\"-\", parameters[[hyp_tune]]$epochs, \"E_run_\", i)\n",
    "\t\tprint(paste(\"Training model\", model_name))\n",
    "\n",
    "\t\t# dir.create(\"TensorBoard_logs\", showWarnings = FALSE)\n",
    "\t\tlog_run_dir <- paste0(\"TensorBoard_logs/tmp/\", model_name)\n",
    "\t\tmodel_checkpoint_dir <- paste0(\"Models/checkpoints/\", model_name)\n",
    "\t\tdir.create(model_checkpoint_dir, showWarnings = FALSE, recursive = TRUE)\n",
    "\t\tmodel_checkpoint_name <- \"/{epoch:02d}E-{val_loss:.5f}vloss.hdf5\"\n",
    "\n",
    "\t\tcallbacks_list = list(\n",
    "\t\t\tcallback_tensorboard(\n",
    "\t\t\t\tlog_dir = log_run_dir,\n",
    "\t\t\t\thistogram_freq = 1 # Records activation histograms every 1 epoch\n",
    "\t\t\t\t# embeddings_freq = 1 # Records embedding data every 1 epoch\n",
    "\t\t\t),\n",
    "\t\t\t# callback_early_stopping(\n",
    "\t\t\t# \tmonitor = \"val_loss\",\n",
    "\t\t\t# \tpatience = 100\n",
    "\t\t\t# ),\n",
    "\t\t\tcallback_model_checkpoint(\n",
    "\t\t\t\tfilepath = paste0(model_checkpoint_dir, \"/\", model_checkpoint_name),\n",
    "\t\t\t\tmonitor = \"val_loss\"\n",
    "\t\t\t\t# save_best_only = TRUE # uncomment if u want to checkpoint only the best models, but when it's commented out, all models will be written (provided the model_checkpoint_dir name for each model varies w/ next epoch..)\n",
    "\t\t\t)\n",
    "\t\t)\n",
    "\n",
    "\t\thistory <- model %>% fit(\n",
    "\t\t\tdata_train, labels_train,\n",
    "\t\t\tepochs = parameters[[hyp_tune]]$epochs, batch_size = parameters[[hyp_tune]]$batch_size, # batch_size = 128, epochs = 500 for real run; epochs = i*ep for increasing number of epochs w/ iterations\n",
    "\t\t\tvalidation_data = list(data_val, labels_val), # computes loss and accuracy for validation data after every epoch\n",
    "\t\t\t# validation_split = 0.2,\n",
    "\t\t\tcallbacks = callbacks_list, # for TensorBoard logging\n",
    "\t\t\t# initial_epoch = 100, # TBD try, integer, Epoch at which to start training (useful for resuming a previous training run).\n",
    "\t\t\tverbose = 2\n",
    "\t\t)\n",
    "\n",
    "\t\t# dir.create(\"Workspaces/runs\", showWarnings = FALSE, recursive = TRUE)\n",
    "\t\tsave(what = model, name = model_name)\n",
    "\t}\n",
    "# }\n",
    "\n",
    "plot(history)\n",
    "\n",
    "\n",
    "#####################\n",
    "####### Notes #######\n",
    "#####################\n",
    "\n",
    "# TBD:\n",
    "\t# extensive hyperpar tuning once the code is scalable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
